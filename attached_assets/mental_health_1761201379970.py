# -*- coding: utf-8 -*-
"""Mental Health.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12xi6gB6WBfsR6Vr1ttMj00PD6bbsMyoG
"""

# import os
# import shutil
# shutil.rmtree("/content/drive/MyDrive/Colab Notebooks/Mental health/Models/prod_ensemble_v1/xlnet")

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""# **Uninstalling Libraries**"""

!pip uninstall -q -y transformers tokenizers huggingface_hub peft accelerate
!pip uninstall -q -y torch torchvision torchaudio numpy

"""# **Installing Required Libraries**"""

# !pip install -q optuna

# for CPU
!pip install -q pandas
!pip install -q numpy==1.25.2
!pip install -q torch==2.1.2 torchvision torchaudio
!pip install -q transformers==4.39.2 datasets==2.19.0 scikit-learn
!pip install -q -U accelerate==0.28.0
!pip install -q gensim

# Core deps with CUDA support (Colab's A100 runs CUDA 12.1)
# !pip install -q torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --index-url https://download.pytorch.org/whl/cu121
# !pip install -q transformers==4.42.3 datasets==2.20.0 scikit-learn accelerate==0.30.1 pandas numpy gensim

!pip uninstall -y torch torchvision torchaudio numpy transformers tokenizers huggingface_hub peft accelerate safetensors

"""# **Importing Libraries**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import Dataset
import torch
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.nn.functional import softmax
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm # This is used for progress bar
from sentence_transformers import SentenceTransformer, util
# from gensim.models import FastText

"""# **Scraping Reddit Posts Using API**"""

!pip install -q asyncpraw

os.environ["REDDIT_CLIENT_ID"] = "5QVAqc3BKPDAnLCSlffqCg"
os.environ["REDDIT_CLIENT_SECRET"] = "Hqbxwh0tBDBM7o8wfSkfKkBwHTeugA"
os.environ["REDDIT_USER_AGENT"] = "MentalHealthBot/G.0"

import os
import pandas as pd
import asyncio
import asyncpraw

async def fetch_reddit_posts():
    reddit = asyncpraw.Reddit(
        client_id=os.getenv("REDDIT_CLIENT_ID"),
        client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
        user_agent=os.getenv("REDDIT_USER_AGENT"),
    )
    reddit.read_only = True
    # ---------------------------------Subbreddit Distribution for training data-----------------
    # subreddit_limits = {
    #     # High-Risk: 4 × 500 = 2000
    #     "SuicideWatch": 500,
    #     "selfharm": 500,
    #     "BPD": 500,
    #     "Psychosis": 500,

    #     # Moderate-Risk: 9 × 300 = 2700
    #     "depression": 300,
    #     "addiction": 300,
    #     "BipolarReddit": 300,
    #     "ptsd": 300,
    #     "cptsd": 300,
    #     "Anxiety": 300,
    #     "Insecure": 300,
    #     "Lonely": 300,
    #     "OCD": 300,

    #     # Mixed: 3 × 300 = 900
    #     "mentalhealth": 300,
    #     "offmychest": 300,
    #     "CasualConversation": 300,

    #     # Positive/No-Risk: 8 × 500 = 4000
    #     "Happiness": 500,
    #     "Happy": 500,
    #     "KindVoice": 500,
    #     "GetMotivated": 500,
    #     "DecidingToBeBetter": 500,
    #     "MadeMeSmile": 500,
    #     "UpliftingNews": 500,
    #     "HumansBeingBros": 500,
    # }
    posts = []
    for sub, limit in subreddit_limits.items():
        subreddit = await reddit.subreddit(sub)
        async for post in subreddit.new(limit=limit):
            posts.append({
                "post_id": post.id,
                "timestamp": post.created_utc,
                "title": post.title,
                "content": post.selftext,
                "subreddit": sub,
                "score": post.score,
                "num_comments": post.num_comments,
                "url": post.url
            })

    df = pd.DataFrame(posts)
    df.to_csv("reddit_mental_health_posts.csv", index=False)
    print(f"CSV file created with {len(df)} rows!")
    return df

df = await fetch_reddit_posts()

"""# **Dataset Cleaning**"""

df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Mental health/Datasets/combined_df")

df.shape

data = df.copy()

data["timestamp"] = pd.to_datetime(data["timestamp"], unit="s")

data["timestamp"].head()

!pip install -q emoji

import re
import emoji
import html

"""#Cleaning Dataset for BERT Sentiment Classification"""

data = data.dropna(subset=['content'])

def clean_text(text):
    """
    Clean text for optimal BERT sentiment analysis.
    Keeps punctuation, capitalization, and emojis where useful.
    """
    if not isinstance(text, str):  # Add this line to handle NaN or float
        return ""
    text = html.unescape(text)
    text = emoji.demojize(text, delimiters=(" ", " "))
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#(\w+)", r"\1", text)  # Keep hashtag words
    text = re.sub(r"\s+", " ", text).strip()

    return text

data["content"] = df["content"].apply(clean_text)
data["content"] = data["title"] + " " + data["content"]

"""# **Classifying Sentiments**
Model - cardiffnlp/twitter-roberta-base-sentiment - This is a pre-trained model from hugging face which is trained using thousands of tweets so it can guess whether a new tweet is positive, negative, or neutral. Here, it is used to classify sentiments of Reddit Posts.
This model works on probability. It finds an output(called logits) for a text and convert them into probabilities using softmax. The sentiment(positive,negative,neutral) having highest probability for a text will be chosed.
"""

MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME) # Load the model
model.eval()

id2label = {0: "negative", 1: "neutral", 2: "positive"} #Labeling Sentiments as 0,1,2

data['content'] = data['content'].fillna("")

sentiment_labels = []
sentiment_scores = []

# Loop through texts and classify
for text in tqdm(data['content'], desc="Classifying Sentiments"):
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = softmax(outputs.logits, dim=1)
            conf, pred = torch.max(probs, dim=1)

        sentiment_labels.append(id2label[pred.item()])
        sentiment_scores.append(round(conf.item(), 4))

    except Exception as e:
        sentiment_labels.append("error")
        sentiment_scores.append(0.0)
        print(f"Error processing: {text[:40]}... | {e}")

data['bert_sentiment'] = sentiment_labels
data['bert_sentiment_confidence'] = sentiment_scores

data.to_csv("bert_sentiment_labeled_1.csv", index=False)

"""# **Preparing Dataset of bert model**"""

data=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Mental health/Datasets/final_df.csv")

# data=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/final_df (1).csv")

data.info()

# data = data[["timestamp","title","content","subreddit","score","num_comments","bert_sentiment","bert_sentiment_confidence"]].dropna()

print(data['bert_sentiment'].isna().sum())

data = data.dropna(subset=['content'])

data['bert_sentiment'].value_counts()

"""# **The function below maps the risk(high,moderate,low,no risk) label with posts using sentiments. This is just the rough labelling of posts.**


*   neutral -> moderate risk
*   negative -> high risk
*   if the sentiment is positive then, then the sentiment confidenct will be further checked,if it is >=0.85 then it will be labeled as no risk and if not then low risk


"""

def map_to_risk_level(row):
    sentiment = row['bert_sentiment']
    confidence = row['bert_sentiment_confidence']

    if sentiment == 'positive':
        if confidence >= 0.50:
            return 'no risk'
        else:
            return 'low'
    elif sentiment == 'neutral':
        return 'moderate'
    elif sentiment == 'negative':
        return 'high'
    else:
        return None  # in case there's missing or unclassified sentiment

data['label'] = data.apply(map_to_risk_level, axis=1)

# Encode labels
label2id = {"low": 0, "moderate": 1, "high": 2, "no risk": 3}
id2label = {v: k for k, v in label2id.items()}
data["label_id"] = data["label"].map(label2id)

print(data['content'].isna().sum())

data['bert_sentiment'].value_counts()

data.info()

# First, do a proper single split
from sklearn.model_selection import train_test_split

# Split into train+val and test
X_temp, X_test, y_temp, y_test = train_test_split(
    data["content"].tolist(),
    data["label_id"].tolist(),
    test_size=0.2,
    stratify=data["label_id"],
    random_state=42
)

# Split train+val into train and validation
X_train, X_val, y_train, y_val = train_test_split(
    X_temp,
    y_temp,
    test_size=0.25,  # 0.25 * 0.8 = 0.2 of original data
    stratify=y_temp,
    random_state=42
)

# # Data Splitting
# train_texts, val_texts, train_labels, val_labels = train_test_split(
#     data["content"].tolist(),
#     data["label_id"].tolist(),
#     test_size=0.2,
#     stratify=data["label_id"],
#     random_state=42
# )
# # X = data['content'].values
# # y = data['label_id'].values

# # X_train, X_temp, y_train, y_temp = train_test_split(
# #     X, y, test_size=0.3, stratify=y, random_state=42
# # )
# # X_val, X_test, y_val, y_test = train_test_split(
# #     X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
# # )
# # Data Splitting (60% train, 20% val, 20% test)
# from sklearn.model_selection import train_test_split

# X_temp, X_test, y_temp, y_test = train_test_split(
#     data["content"], data["label_id"],
#     test_size=0.2, stratify=data["label_id"], random_state=42
# )

# X_train, X_val, y_train, y_val = train_test_split(
#     X_temp, y_temp,
#     test_size=0.25, stratify=y_temp, random_state=42
# )
# # Now you have: 60% train, 20% val, 20% test

"""# **Training Model**

# **bert-base-uncased**
"""

# tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

# #  Converting dataset into PyTorch dataset format because hugging face expects it to be in thta format
# class MentalHealthDataset(Dataset):
#     def __init__(self, encodings, labels):
#         self.encodings = encodings
#         self.labels = labels

#     def __len__(self):
#         return len(self.labels)

#     def __getitem__(self, idx):
#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
#         item["labels"] = torch.tensor(self.labels[idx])
#         return item

# train_dataset = MentalHealthDataset(train_encodings, train_labels)
# val_dataset = MentalHealthDataset(val_encodings, val_labels)

# model = BertForSequenceClassification.from_pretrained(
#     "bert-base-uncased",
#     num_labels=4,
#     id2label=id2label,
#     label2id=label2id
# )

# training_args = TrainingArguments(
#     output_dir="./results",
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=2e-5,
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     num_train_epochs=4,
#     weight_decay=0.01,
#     load_best_model_at_end=True,
#     metric_for_best_model="f1"
# )

# from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = logits.argmax(axis=-1)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
#     acc = accuracy_score(labels, predictions)
#     return {
#         'accuracy': acc,
#         'f1': f1,
#         'precision': precision,
#         'recall': recall
#     }

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     tokenizer=tokenizer,
#     compute_metrics=compute_metrics
# )

# trainer.train()

# trainer.save_model("./reddit_mental_health_risk_model")
# tokenizer.save_pretrained("./reddit_mental_health_risk_model")

"""# **roberta-base**"""

# tokenizer = AutoTokenizer.from_pretrained("roberta-base")
# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

# #  Converting dataset into PyTorch dataset format because hugging face expects it to be in thta format
# class MentalHealthDataset(Dataset):
#     def __init__(self, encodings, labels):
#         self.encodings = encodings
#         self.labels = labels

#     def __len__(self):
#         return len(self.labels)

#     def __getitem__(self, idx):
#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
#         item["labels"] = torch.tensor(self.labels[idx])
#         return item

# train_dataset = MentalHealthDataset(train_encodings, train_labels)
# val_dataset = MentalHealthDataset(val_encodings, val_labels)

# model = BertForSequenceClassification.from_pretrained(
#     "roberta-base",
#     num_labels=4,
#     id2label=id2label,
#     label2id=label2id
# )

# from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = logits.argmax(axis=-1)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
#     acc = accuracy_score(labels, predictions)
#     return {
#         'accuracy': acc,
#         'f1': f1,
#         'precision': precision,
#         'recall': recall
#     }

# training_args = TrainingArguments(
#     output_dir="./results",
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=2e-5,
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     num_train_epochs=4,
#     weight_decay=0.01,
#     load_best_model_at_end=True,
#     metric_for_best_model="f1"
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     tokenizer=tokenizer,
#     compute_metrics=compute_metrics
# )

# trainer.train()

# trainer.save_model("./roberta_base_risk_model")
# tokenizer.save_pretrained("./roberta_base_risk_model")

"""# **xlnet-base-cased**"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
model = AutoModelForSequenceClassification.from_pretrained("xlnet-base-cased", num_labels=4, id2label=id2label, label2id=label2id)

#  Converting dataset into PyTorch dataset format because hugging face expects it to be in thta format
class MentalHealthDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_encodings = tokenizer(X_temp, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)

train_dataset = MentalHealthDataset(train_encodings, y_temp)
val_dataset = MentalHealthDataset(val_encodings, y_test)

model = AutoModelForSequenceClassification.from_pretrained(
    "xlnet-base-cased",
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2.6237146630184746e-05,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=4,
    weight_decay=0.0767760827226268,
    warmup_ratio= 0.08293158925167873,
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

best_ckpt_path = "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/xlnet/results/checkpoint-18344"

# Load the model and tokenizer from the checkpoint
model = AutoModelForSequenceClassification.from_pretrained(best_ckpt_path)
tokenizer = AutoTokenizer.from_pretrained(best_ckpt_path)

# Save to a clean folder
model.save_pretrained("./xlnet_risk_model")
tokenizer.save_pretrained("./xlnet_risk_model")

trainer.save_model("./xlnet_risk_model")
tokenizer.save_pretrained("./xlnet_risk_model")

"""# **mental-bert-base-uncased**"""

# from huggingface_hub import login
# login("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")

# from transformers import AutoTokenizer, AutoModelForSequenceClassification

# tokenizer = AutoTokenizer.from_pretrained("mental/mental-bert-base-uncased")
# model = AutoModelForSequenceClassification.from_pretrained("mental/mental-bert-base-uncased", num_labels=4, id2label=id2label, label2id=label2id)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

class MentalHealthDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = MentalHealthDataset(train_encodings, train_labels)
val_dataset = MentalHealthDataset(val_encodings, val_labels)

model = AutoModelForSequenceClassification.from_pretrained(
    "mental/mental-bert-base-uncased",
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=4,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model("./mental-bert_risk_model")
tokenizer.save_pretrained("./mental-bert_risk_model")

"""# **mental-RoBERTa-large**"""

# # @title
# from huggingface_hub import login
# login("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")

# from transformers import AutoTokenizer, AutoModelForSequenceClassification

# tokenizer = AutoTokenizer.from_pretrained("AIMH/mental-roberta-large")
# model = AutoModelForSequenceClassification.from_pretrained("AIMH/mental-roberta-large", num_labels=4, id2label=id2label, label2id=label2id)

# train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)
# val_encodings   = tokenizer(list(X_val), truncation=True, padding=True, max_length=128)
# test_encodings  = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)

# class MentalHealthDataset(torch.utils.data.Dataset):
#     def __init__(self, encodings, labels):
#         self.encodings = encodings
#         self.labels = labels

#     def __len__(self):
#         return len(self.labels)

#     def __getitem__(self, idx):
#         item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
#         item["labels"] = torch.tensor(self.labels[idx])
#         return item

# train_dataset = MentalHealthDataset(train_encodings, list(y_train))
# val_dataset   = MentalHealthDataset(val_encodings, list(y_val))
# test_dataset  = MentalHealthDataset(test_encodings, list(y_test))

# model = AutoModelForSequenceClassification.from_pretrained(
#     "AIMH/mental-roberta-large",
#     num_labels=4,
#     id2label=id2label,
#     label2id=label2id
# )

# from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = logits.argmax(axis=-1)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
#     acc = accuracy_score(labels, predictions)
#     return {
#         'accuracy': acc,
#         'f1': f1,
#         'precision': precision,
#         'recall': recall
#     }

# training_args = TrainingArguments(
#     output_dir="./results",
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=1.937731987730832e-05,
#     per_device_train_batch_size=4,
#     per_device_eval_batch_size=4,
#     gradient_accumulation_steps=4,
#     num_train_epochs=4,
#     weight_decay=0.04881607404316071,
#     warmup_ratio=0.09581542163949541,
#     load_best_model_at_end=True,
#     metric_for_best_model="f1",
#     save_safetensors=True
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     tokenizer=tokenizer,
#     compute_metrics=compute_metrics
# )

# trainer.train()

# best_ckpt_path = "/content/results/checkpoint-9172"

# # Load the model and tokenizer from the checkpoint
# model = AutoModelForSequenceClassification.from_pretrained(best_ckpt_path)
# tokenizer = AutoTokenizer.from_pretrained(best_ckpt_path)

# # Save to a clean folder
# model.save_pretrained("./AIMH-mental-roberta", safe_serialization=True)
# tokenizer.save_pretrained("./AIMH-mental-roberta")

# trainer.save_model("./AIMH-mental-roberta", safe_serialization=True)
# tokenizer.save_pretrained("./AIMH-mental-roberta")

"""# **DeBERTa-v3-large**"""

# from transformers import AutoTokenizer, AutoModelForSequenceClassification

# tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-large")
# model = AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-v3-large", num_labels=4, id2label=id2label, label2id=label2id)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

class MentalHealthDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = MentalHealthDataset(train_encodings, train_labels)
val_dataset = MentalHealthDataset(val_encodings, val_labels)

model = AutoModelForSequenceClassification.from_pretrained(
    "microsoft/deberta-v3-large",
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=4,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

"""Now, i am using the models below for ensemble

XL-BERT

DistilBERT

mental-RoBERTa-large

# **DistillBERT**
"""

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=4, id2label=id2label, label2id=label2id)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

class MentalHealthDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = MentalHealthDataset(train_encodings, train_labels)
val_dataset = MentalHealthDataset(val_encodings, val_labels)

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
        output_dir=f"./results",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate= 4.64248118508974e-05,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        num_train_epochs=5,
        weight_decay=0.054354714266954006,
        warmup_ratio= 0.19725010465961618,
        logging_steps=10,
        load_best_model_at_end=False,
        report_to="none",
        save_safetensors=True
    )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model("./distill-bert", safe_serialization=True)
tokenizer.save_pretrained("./distill-bert", safe_serialization=True)

"""# **Optuna Hyperparameter Tuning for the choosed models**"""

# @title
import optuna

# @title
class MentalHealthDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item
def compute_metrics(eval_pred):
  logits, labels = eval_pred
  predictions = logits.argmax(axis=-1)
  precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
  acc = accuracy_score(labels, predictions)
  return {
      'accuracy': acc,
      'f1': f1,
      'precision': precision,
      'recall': recall
  }

# @title
def objective(trial):
    # Hyperparameters to tune - NOTE THE CONSISTENT NAMING
    params = {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32]),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 3, 5),
        "weight_decay": trial.suggest_float("weight_decay", 0.01, 0.1),
        "warmup_ratio": trial.suggest_float("warmup_ratio", 0.05, 0.2),
    }

    # Model selection
    model_name = trial.suggest_categorical("model_name", [
        # "xlnet-base-cased",
        "distilbert-base-uncased",
        # "AIMH/mental-roberta-large"
    ])

    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=4,
        id2label=id2label,
        label2id=label2id
    )

    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)

    train_dataset = MentalHealthDataset(train_encodings, train_labels)
    val_dataset = MentalHealthDataset(val_encodings, val_labels)

    gradient_checkpointing = False if "xlnet" in model_name.lower() else True

    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        save_strategy="no",
        learning_rate=2e-5,                   # Start with your best manual LR
        per_device_train_batch_size=8,        # Keep batch size small
        per_device_eval_batch_size=8,
        num_train_epochs=4,
        weight_decay=0.01,
        load_best_model_at_end=False,
        metric_for_best_model="f1",
        logging_steps=50,                     # Log training loss
        fp16=True,                            # Enable mixed precision
        warmup_ratio=0.1,                     # Add warmup
    )
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    trainer.train()
    eval_result = trainer.evaluate()
    train_loss = None
    for record in trainer.state.log_history[::-1]:
        if "loss" in record:
            train_loss = record["loss"]
            break

    # Return multiple metrics packed in a dictionary
    trial.set_user_attr("model_name", model_name)
    trial.set_user_attr("accuracy", eval_result.get("eval_accuracy"))
    trial.set_user_attr("precision", eval_result.get("eval_precision"))
    trial.set_user_attr("recall", eval_result.get("eval_recall"))
    trial.set_user_attr("train_loss", train_loss)
    trial.set_user_attr("val_loss", eval_result.get("eval_loss"))

    return eval_result["eval_f1"]

# @title
from huggingface_hub import login
login("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")

# @title
from collections import defaultdict

model_results = defaultdict(list)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=10)  # Start with 10 trials

# Collect trials
for t in study.trials:
    if t.state == optuna.trial.TrialState.COMPLETE:
        model = t.user_attrs["model_name"]
        f1 = t.value
        model_results[model].append({
            "trial_number": t.number,
            "f1": f1,
            "accuracy": t.user_attrs.get("accuracy"),
            "precision": t.user_attrs.get("precision"),
            "recall": t.user_attrs.get("recall"),
            "train_loss": t.user_attrs.get("train_loss"),
            "val_loss": t.user_attrs.get("val_loss"),
            "params": t.params
        })

# Print All Configurations Per Model
print("\n================= ALL CONFIGURATIONS PER MODEL =================\n")
for model, trials in model_results.items():
    print(f"\nModel: {model}")
    print(f"{'-'*70}")
    sorted_trials = sorted(trials, key=lambda x: x["f1"], reverse=True)
    for trial in sorted_trials:
        print(f"Trial #{trial['trial_number']} | F1: {trial['f1']:.4f} | Acc: {trial['accuracy']:.4f} | Prec: {trial['precision']:.4f} | Rec: {trial['recall']:.4f} | Train Loss: {trial['train_loss']:.4f} | Val Loss: {trial['val_loss']:.4f}")
        print("Hyperparameters:")
        for param_name, param_value in trial["params"].items():
            if param_name != "model_name":
                print(f"  {param_name}: {param_value}")
        print("-"*50)

# Print Best Config Per Model
print("\n================= BEST CONFIGURATION PER MODEL =================\n")
for model, trials in model_results.items():
    best_trial = max(trials, key=lambda x: x["f1"])
    print(f"Model: {model}")
    print(f"Best F1: {best_trial['f1']:.4f} | Acc: {best_trial['accuracy']:.4f} | Prec: {best_trial['precision']:.4f} | Rec: {best_trial['recall']:.4f} | Train Loss: {best_trial['train_loss']:.4f} | Val Loss: {best_trial['val_loss']:.4f}")
    print("Hyperparameters:")
    for param_name, param_value in best_trial["params"].items():
        if param_name != "model_name":
            print(f"  {param_name}: {param_value}")
    print("="*70)

"""# **Training models on best Parameters**
parameters written in the above models are best parameters came after tuning

# **Ensemble**
"""

import os
import torch
import numpy as np
import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from torch.cuda.amp import autocast
from tqdm import tqdm

# class MentalHealthEnsemble:
#     def __init__(self, X_val=None, y_val=None):
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#         os.environ["TOKENIZERS_PARALLELISM"] = "true"

#         self.model_paths = {
#             "xlnet": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/xlnet/xlnet_risk_model",
#             "distilbert": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/distill-bert/checkpoint-4586",
#             "mental-roberta": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/AIMH_mental_roberta/checkpoint-6878"
#             # "xlnet": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/xlnet/xlnet_risk_model",
#             # "distilbert": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/distill-bert/checkpoint-4586",
#             # "mental-roberta": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/AIMH_mental_roberta/checkpoint-6878"
#         }

#         self.models = self._load_models()
#         self.meta_model = None
#         self.val_metrics = None

#         # Define label mappings
#         self.label2id = {"low": 0, "moderate": 1, "high": 2, "no risk": 3}
#         self.id2label = {v: k for k, v in self.label2id.items()}

#         # Initialize validation data if provided
#         if X_val is not None and y_val is not None:
#             self.X_val, self.y_val = self._ensure_text_format(X_val, y_val)
#         else:
#             self.X_val, self.y_val = None, None

#     def _ensure_text_format(self, texts, labels=None):
#         if isinstance(texts, (np.ndarray, pd.Series)):
#             texts = texts.tolist()
#         texts = [str(x) if pd.notna(x) else "" for x in texts]

#         if labels is not None:
#             if isinstance(labels, (np.ndarray, pd.Series)):
#                 labels = labels.tolist()
#             return texts, labels
#         return texts

#     def _load_models(self):
#         models = {}
#         for name, path in self.model_paths.items():
#             try:
#                 print(f"Loading {name} from {path}...")
#                 model = AutoModelForSequenceClassification.from_pretrained(path)
#                 tokenizer = AutoTokenizer.from_pretrained(path)
#                 models[name] = {
#                     "model": model.to(self.device).eval(),
#                     "tokenizer": tokenizer
#                 }
#                 print(f"✅ {name} loaded successfully")
#             except Exception as e:
#                 print(f"❌ Failed to load {name}: {str(e)}")
#                 continue
#         return models

#     def _predict_batch(self, model, tokenizer, texts, max_length=128, batch_size=8):
#         """Memory-safe batch prediction with progress bar"""
#         texts = self._ensure_text_format(texts)
#         all_probs = []

#         for i in tqdm(range(0, len(texts), batch_size), desc="Batch prediction", leave=False):
#             batch_texts = texts[i:i + batch_size]
#             inputs = tokenizer(
#                 batch_texts,
#                 return_tensors="pt",
#                 padding=True,
#                 truncation=True,
#                 max_length=max_length
#             ).to(self.device)

#             with torch.no_grad():
#                 with autocast():
#                     outputs = model(**inputs)
#                     probs = torch.nn.functional.softmax(outputs.logits, dim=1)

#             all_probs.append(probs.cpu().numpy())
#             del inputs, outputs, probs
#             torch.cuda.empty_cache()

#         return np.vstack(all_probs)

#     def train_meta_model(self, X_train, y_train):
#         """Train logistic regression meta-model with memory-safe prediction"""
#         print("Training meta-model...")
#         meta_features = []

#         # Convert to proper format
#         X_train, y_train = self._ensure_text_format(X_train, y_train)

#         for name, model_info in tqdm(self.models.items(), desc="Processing base models", unit="model"):
#             probs = self._predict_batch(
#                 model_info["model"],
#                 model_info["tokenizer"],
#                 X_train,
#                 max_length=128,
#                 batch_size=8
#             )
#             meta_features.append(probs)
#             torch.cuda.empty_cache()

#         meta_X = np.hstack(meta_features)

#         self.meta_model = LogisticRegression(
#             multi_class='multinomial',
#             max_iter=1000,
#             class_weight='balanced',
#             solver='lbfgs'
#         )
#         self.meta_model.fit(meta_X, y_train)
#         print("✅ Meta-model training complete")

#     def predict(self, texts, max_length=128):
#         if isinstance(texts, str):
#             texts = [texts]

#         texts = self._ensure_text_format(texts)
#         all_probs = []

#         for name, model_info in self.models.items():
#             try:
#                 probs = self._predict_batch(
#                     model_info["model"],
#                     model_info["tokenizer"],
#                     texts,
#                     max_length,
#                     batch_size=8
#                 )
#                 all_probs.append(probs)
#             except Exception as e:
#                 print(f"⚠️ Skipping {name}: {str(e)}")
#                 continue

#         if not all_probs:
#             raise RuntimeError("All models failed - cannot make predictions")

#         if self.meta_model:
#             # Use meta-model for prediction
#             stacked_probs = np.hstack(all_probs)
#             predictions = self.meta_model.predict(stacked_probs)
#             confidences = np.max(self.meta_model.predict_proba(stacked_probs), axis=1)
#         else:
#             # Fallback to simple averaging
#             avg_probs = np.mean(all_probs, axis=0)
#             predictions = np.argmax(avg_probs, axis=1)
#             confidences = np.max(avg_probs, axis=1)

#         risk_labels = [self.id2label.get(p, "unknown") for p in predictions]
#         return risk_labels, confidences.tolist()
class MentalHealthEnsemble:
    def __init__(self, X_val=None, y_val=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        os.environ["TOKENIZERS_PARALLELISM"] = "true"

        self.model_paths = {
            # A id
            "xlnet": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/xlnet/checkpoint-27516",
            "distilbert": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/distill-bert/checkpoint-4586",
            "mental-roberta": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/AIMH_mental_roberta/checkpoint-6878"
            # Gunjan's id
            # "xlnet": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/xlnet/checkpoint-27516",
            # "distilbert": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/distill-bert/checkpoint-4586",
            # "mental-roberta": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/AIMH_mental_roberta/checkpoint-6878"

        }


        self.models = self._load_models()
        self.meta_model = None
        self.val_metrics = None
        self.confidence_thresholds = {
            'no_risk': 0.85,
            'low': 0.70,
            'moderate': 0.65,
            'high': 0.75
        }
        self.label2id = {"low": 0, "moderate": 1, "high": 2, "no risk": 3}
        self.id2label = {v: k for k, v in label2id.items()}
        if X_val is not None:
            self._initialize_validation(*self._ensure_text_format(X_val, y_val))

    def _ensure_text_format(self, texts, labels=None):
        """Convert input to list of strings and handle NaN values"""
        if isinstance(texts, (np.ndarray, pd.Series)):
            texts = texts.tolist()
        texts = [str(x) if pd.notna(x) else "" for x in texts]

        if labels is not None:
            if isinstance(labels, (np.ndarray, pd.Series)):
                labels = labels.tolist()
            return texts, labels
        return texts

    def _load_models(self):
        """Load models with better error handling"""
        models = {}
        for name, path in self.model_paths.items():
            try:
                print(f"Loading {name} from {path}...")
                model = AutoModelForSequenceClassification.from_pretrained(path)
                tokenizer = AutoTokenizer.from_pretrained(path)
                models[name] = {
                    "model": model.to(self.device).eval(),
                    "tokenizer": tokenizer
                }
                print(f"✅ {name} loaded successfully")
            except Exception as e:
                print(f"❌ Failed to load {name}: {str(e)}")
                continue
        return models


    def _initialize_validation(self, X_val, y_val):
        """Initialize validation metrics"""
        print("Computing validation metrics...")
        val_preds = {}
        val_probs = {}

        for name, model_info in self.models.items():
            probs = self._predict_batch(model_info["model"], model_info["tokenizer"], X_val, 128)
            val_preds[name] = np.argmax(probs, axis=1)
            val_probs[name] = probs

        self.val_metrics = {
            name: {
                'probs': val_probs[name],
                'preds': val_preds[name]
            }
            for name in self.models
        }

    def train_meta_model(self, X_train, y_train, X_val=None, y_val=None):
        """Trains logistic regression meta-model"""
        print("Training meta-model...")

        # Generate meta-features
        meta_features = []
        for name, model_info in self.models.items():
            probs = self._predict_batch(
                model_info["model"], model_info["tokenizer"], X_train, 128
            )
            meta_features.append(probs)
        meta_X = np.hstack(meta_features)

        # Initialize and train meta-model
        self.meta_model = LogisticRegression(
            multi_class='multinomial',
            max_iter=1000,
            class_weight='balanced',
            solver='lbfgs'
        )
        self.meta_model.fit(meta_X, y_train)


    def _predict_batch(self, model, tokenizer, texts, max_length):
        """Batch prediction helper"""
        texts = self._ensure_text_format(texts)
        inputs = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(self.device)

        with torch.no_grad():
            outputs = model(**inputs)
            return torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()


    def predict(self, texts, max_length=128):
      """
      Predicts risk levels with confidence scores.
      Returns: (predictions, confidences) where:
          - predictions: List of risk labels ('high', 'moderate', etc.)
          - confidences: List of confidence scores (0-1)
      """
      # Handle single text input
      if isinstance(texts, str):
          texts = [texts]

      # Get probabilities from all available models
      all_probs = []
      for name, model_info in self.models.items():
          try:
              probs = self._predict_batch(model_info["model"], model_info["tokenizer"], texts, max_length)
              all_probs.append(probs)
          except Exception as e:
              print(f"⚠️ Skipping {name}: {str(e)}")
              continue

      if not all_probs:
          raise RuntimeError("All models failed - cannot make predictions")

      # Stacking (if meta-model exists)
      if self.meta_model:
          stacked_probs = np.hstack(all_probs)
          predictions = self.meta_model.predict(stacked_probs)
          # Confidence = max average probability across models
          confidences = np.max(np.mean(all_probs, axis=0), axis=1)
      # Weighted average (fallback)
      else:
          avg_probs = np.mean(all_probs, axis=0)
          predictions = np.argmax(avg_probs, axis=1)
          confidences = np.max(avg_probs, axis=1)

      # Convert to readable labels
      risk_labels = [self.id2label[p] for p in predictions]

      return risk_labels, confidences.tolist()

"""# **Training and Saving the model**"""

# Initialize ensemble with validation data for calibration
ensemble = MentalHealthEnsemble(X_val, y_val)

# Train meta-model with training data
# ensemble.train_meta_model(X_train, y_train)

def save_ensemble(ensemble, save_dir):
    """
    Save all components of the ensemble:
    - Base Hugging Face models & tokenizers
    - Meta-model (Logistic Regression)
    - Validation metrics
    - Config & metadata
    """
    import os
    import torch
    import numpy as np
    from joblib import dump

    os.makedirs(save_dir, exist_ok=True)

    # 1. Save base models + tokenizers
    relative_model_paths = {}
    for model_name, model_info in ensemble.models.items():
        model_dir = os.path.join(save_dir, f"model_{model_name}")
        os.makedirs(model_dir, exist_ok=True)
        model_info["model"].save_pretrained(model_dir)
        model_info["tokenizer"].save_pretrained(model_dir)
        relative_model_paths[model_name] = f"model_{model_name}"

    # 2. Save meta-model
    if ensemble.meta_model is not None:
        dump(ensemble.meta_model, os.path.join(save_dir, "meta_model.joblib"))

    # 3. Save validation metrics
    if ensemble.val_metrics is not None:
        np.save(os.path.join(save_dir, "val_metrics.npy"), ensemble.val_metrics, allow_pickle=True)

    # 4. Save metadata (NOW includes thresholds too)
    metadata = {
        'model_paths': relative_model_paths,
        'confidence_thresholds': getattr(ensemble, 'confidence_thresholds', None),
        'id2label': getattr(ensemble, 'id2label', None),
        'label2id': getattr(ensemble, 'label2id', None)
    }
    torch.save(metadata, os.path.join(save_dir, "ensemble_metadata.pt"))

    print(f"✅ Ensemble successfully saved to {save_dir}")
    return save_dir

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss

def evaluate_ensemble(ensemble, X_train, y_train, X_val, y_val):
    # ---- Training predictions ----
    train_probs = []
    for name, model_info in ensemble.models.items():
        probs = ensemble._predict_batch(
            model_info["model"], model_info["tokenizer"], X_train, 128, batch_size=8
        )
        train_probs.append(probs)
    train_meta_X = np.hstack(train_probs)
    train_preds = ensemble.meta_model.predict(train_meta_X)
    train_probs = ensemble.meta_model.predict_proba(train_meta_X)

    # ---- Validation predictions ----
    val_probs_list = []
    for name, model_info in ensemble.models.items():
        probs = ensemble._predict_batch(
            model_info["model"], model_info["tokenizer"], X_val, 128, batch_size=8
        )
        val_probs_list.append(probs)
    val_meta_X = np.hstack(val_probs_list)
    val_preds = ensemble.meta_model.predict(val_meta_X)
    val_probs = ensemble.meta_model.predict_proba(val_meta_X)

    # ---- Collect metrics ----
    results = pd.DataFrame([{
        "Training Loss": log_loss(y_train, train_probs),
        "Validation Loss": log_loss(y_val, val_probs),
        "Training Accuracy": accuracy_score(y_train, train_preds),
        "Validation Accuracy": accuracy_score(y_val, val_preds),
        "Training F1": f1_score(y_train, train_preds, average="macro"),
        "Validation F1": f1_score(y_val, val_preds, average="macro"),
        "Training Precision": precision_score(y_train, train_preds, average="macro"),
        "Validation Precision": precision_score(y_val, val_preds, average="macro"),
        "Training Recall": recall_score(y_train, train_preds, average="macro"),
        "Validation Recall": recall_score(y_val, val_preds, average="macro"),
    }])

    return results

results = evaluate_ensemble(ensemble, X_train, y_train, X_val, y_val)
print(results.to_string(index=False))

# 2. Save to Google Drive
save_path = "/content/drive/MyDrive/Colab Notebooks/Mental health/Models/prod_ensemble_v1"
save_ensemble(ensemble, save_path)

"""# **Loading ensemble**"""

def load_ensemble(save_dir, device=None):
    """Load ensemble with proper handling of sklearn models and defaults"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize empty ensemble
    ensemble = MentalHealthEnsemble.__new__(MentalHealthEnsemble)
    ensemble.device = device

    # Load metadata
    metadata = torch.load(
        os.path.join(save_dir, "ensemble_metadata.pt"),
        weights_only=False
    )

    ensemble.model_paths = {
        "xlnet": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/xlnet/checkpoint-27516",
        "distilbert": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/distill-bert/checkpoint-4586",
        "mental-roberta": "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/AIMH_mental_roberta/checkpoint-6878"
        # Gunjan's id
        # "xlnet": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/xlnet/checkpoint-27516",
        # "distilbert": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/distill-bert/checkpoint-4586",
        # "mental-roberta": "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/AIMH_mental_roberta/checkpoint-6878"
}
    ensemble.id2label = metadata.get('id2label', {0: "low", 1: "moderate", 2: "high", 3: "no_risk"})
    ensemble.label2id = metadata.get('label2id', {v: k for k, v in ensemble.id2label.items()})

    # Reload base models
    ensemble.models = {}
    for name, path in ensemble.model_paths.items():
        try:
            ensemble.models[name] = {
                "model": AutoModelForSequenceClassification.from_pretrained(path).to(device).eval(),
                "tokenizer": AutoTokenizer.from_pretrained(path)
            }
            print(f"✅ Loaded model: {name}")
        except Exception as e:
            print(f"❌ Failed to reload {name}: {str(e)}")
            continue

    # Load meta-model
    meta_model_path = os.path.join(save_dir, "meta_model.joblib")
    if os.path.exists(meta_model_path):
        from joblib import load
        ensemble.meta_model = load(meta_model_path)
    else:
        ensemble.meta_model = None

    # Load validation metrics
    val_metrics_path = os.path.join(save_dir, "val_metrics.npy")
    if os.path.exists(val_metrics_path):
        ensemble.val_metrics = np.load(val_metrics_path, allow_pickle=True).item()
    else:
        ensemble.val_metrics = None

    # Default confidence thresholds
    ensemble.confidence_thresholds = {
        'no_risk': 0.85,
        'low': 0.70,
        'moderate': 0.65,
        'high': 0.75
    }

    print("✅ Ensemble loaded successfully")
    return ensemble

"""# **Using fine tuned model to classify risks**"""

#-----------------Risk Dictionary--------------#
risk_lexicon = {
    "high": [
        # Suicide intent and planning
        "suicide", "kill myself", "end my life", "want to die", "don't want to live anymore",
        "plan to hurt myself", "going to overdose", "can't take it anymore", "final goodbye",
        "no way out", "better off dead", "ending it all", "suicidal thoughts", "ready to die",
        "tired of fighting", "self-harm", "bleeding out", "jumping off", "shooting myself",
        "hanging myself", "pills to die", "no hope left", "worthless existence", "can't go on",
        "unalive myself", "end it tonight", "never wake up", "desperate enough to die",
        "life is unbearable", "done with life", "suicide attempt", "thinking of ways to die",
        "no reason to live", "preparing to die", "giving up on life", "cutting to end it",
        "overdose on purpose", "suicide plan", "written a suicide note", "researching suicide methods",
        "no fear of death anymore", "hopeless and suicidal", "actively suicidal", "fantasizing about death",
        "wish I was dead", "people would miss me if I was gone", "suicidal and no one cares",
        "triggered to kill myself", "methods to die", "ways to commit suicide", "painless suicide",
        "quick ways to die", "suicide prevention bypass", "absolutely hopeless", "completely defeated",

        # Immediate harm and methods
        "overdose on pills", "cutting deep", "bleeding out", "jumping off", "shooting myself",
        "hanging myself", "carbon monoxide", "poison myself", "jump from height", "step in front of train",
        "car exhaust suicide", "drug overdose", "asphyxiation", "drowning myself", "self-inflicted gunshot",
        "wrist cutting", "throat cutting", "vein opening", "artery cutting", "jump from bridge",
        "plastic bag suffocation", "hanging from rope", "chemical poisoning", "household chemical suicide",
        "antifreeze poisoning", "pesticide ingestion", "rat poison consumption", "bleach drinking",

        # Psychotic and command symptoms
        "voices commanding suicide", "demons telling me to die", "god says I must die", "angels want me dead",
        "spirits demanding sacrifice", "must kill myself to save others", "sacrifice myself for world",
        "die to stop apocalypse", "end world suffering by dying", "martyr myself", "death will save everyone",
        "suicide as redemption", "punishment for sins", "deserve to die", "need to be punished",
        "must atone with death", "death is purification", "cleanse through suicide", "ritual suicide",

        # Immediate urgency
        "cutting right now", "bleeding currently", "overdosing at this moment", "about to jump",
        "holding weapon now", "pills in hand", "gun loaded", "standing on edge", "ready to jump",
        "about to end it", "final moment", "doing it now", "acting on impulses", "cannot stop myself",
        "losing control", "impulse to harm", "urge to kill myself", "compelled to die", "forced to suicide",
        "must die today", "today is the day", "now or never", "point of no return", "crossing the line",
        "beyond help", "too late for help", "emergency situation", "critical state", "immediate danger",
        "lifethreatening now", "mortal danger", "perilous situation", "jeopardizing life"
    ],
    "moderate": [
        # Depression descriptors
        "depressed", "hopeless", "empty inside", "numb", "crying every day", "can't stop crying",
        "hate myself", "feel like a burden", "no one cares", "isolated", "helpless", "trapped",
        "drowning in sadness", "broken beyond repair", "mentally exhausted", "life is too hard",
        "why am I even here", "no purpose", "disconnected", "unbearable pain", "emotional agony",
        "can't cope anymore", "overwhelmed by sadness", "thoughts of dying", "passive suicidal",
        "don't care if I die", "maybe I shouldn't exist", "wish I could disappear", "not actively suicidal but",
        "struggling to keep going", "barely holding on", "mentally collapsing", "drained of all hope",
        "feeling like a failure", "lost all motivation", "nothing brings joy anymore", "constant sadness",
        "life feels meaningless", "wishing for an escape", "can't see a future", "dreading every day",
        "feeling hopeless", "mentally unstable", "on the edge of breaking down", "too much to handle",
        "drowning in despair", "feeling invisible", "consumed by sadness", "perpetual gloom",
        "cloud of depression", "weight of despair", "heavy heart always", "permanent sadness",
        "unending misery", "chronic unhappiness", "persistent despair", "ongoing melancholy",

        # Anxiety and panic
        "constant anxiety", "perpetually worried", "always nervous", "can't stop worrying", "racing thoughts",
        "mind won't quiet", "always on edge", "permanently tense", "cannot relax ever", "always anticipating disaster",
        "expecting the worst always", "catastrophic thinking", "doom thinking", "negative anticipation",
        "constant dread", "pervasive fear", "ongoing apprehension", "continuous unease", "persistent nervousness",
        "unrelenting tension", "ceaseless worry", "never calm", "always agitated", "perpetually restless",
        "cannot sit still", "always fidgeting", "constant physical tension", "muscles always tight",
        "jaw always clenched", "always on high alert", "hypervigilant constantly", "startle easily",
        "jumpy all the time", "panic frequently", "regular panic attacks", "anxiety attacks often",
        "frequent meltdowns", "emotional breakdowns regularly", "often overwhelmed", "frequently overstimulated",

        # Self-harm thoughts
        "thinking about cutting", "want to self harm", "urge to hurt myself", "feel like cutting",
        "want to feel physical pain", "need to punish myself", "deserve to suffer", "should be hurt",
        "need to see blood", "want to feel something", "numb need sensation", "emotional pain too much",
        "convert emotional to physical", "pain distraction", "self harm as release", "cutting to cope",
        "burning to feel", "hitting myself", "punching walls", "banging head", "self injury thoughts",
        "self mutilation ideas", "self harm urges", "fighting self harm impulses", "resisting cutting",
        "trying not to hurt myself", "white-knuckling through urges", "self harm cravings",
        "obsessed with self injury", "preoccupied with pain", "fascinated with blood",

        # Substance abuse
        "drinking to cope", "using drugs to escape", "self-medicating with alcohol", "substance abuse daily",
        "dependent on drugs", "reliant on alcohol", "cannot function without substances", "need drugs to cope",
        "require alcohol to sleep", "drinking until blackout", "using until unconscious", "overdosing regularly",
        "binge drinking often", "frequent drug binges", "pattern of substance abuse", "escalating drug use",
        "increasing alcohol consumption", "tolerance building", "withdrawal symptoms", "needing more to get same effect",
        "spiraling addiction", "out of control using", "cannot stop drinking", "powerless over drugs"
    ],
    "low": [
        # General distress
        "sad", "stressed", "anxious", "overwhelmed", "frustrated", "exhausted", "burned out",
        "mentally drained", "feeling down", "in a funk", "low energy", "discouraged", "disappointed",
        "unmotivated", "stuck", "lost", "confused", "unsure about life", "questioning everything",
        "needing a break", "worn out", "emotionally tired", "need support", "going through a rough patch",
        "feeling off", "not myself lately", "mood swings", "irritable", "foggy mind", "can't focus",
        "sleep problems", "appetite changes", "withdrawing a bit", "less social", "needing space",
        "overthinking", "self-doubt", "comparing myself to others", "feeling inadequate", "minor regrets",
        "wish things were different", "fear of failure", "general worry", "nervous about the future",
        "missing someone", "lonely at times", "needing encouragement", "seeking clarity", "looking for direction",
        "uncertain about path", "questioning decisions", "doubting choices", "second guessing",

        # Mild anxiety
        "a bit anxious", "slightly nervous", "somewhat worried", "mild apprehension", "minor concerns",
        "small worries", "everyday anxiety", "normal nervousness", "typical stress", "common worries",
        "routine anxiety", "expected nervousness", "situational anxiety", "context-specific worry",
        "temporary nervousness", "short-term anxiety", "passing worry", "fleeting concern",
        "momentary apprehension", "brief stress", "transient anxiety", "occasional worry",
        "intermittent nervousness", "periodic anxiety", "cyclic worry", "seasonal stress",

        # Life challenges
        "work stress", "job pressure", "career concerns", "professional challenges", "workload issues",
        "deadline pressure", "performance worries", "job security concerns", "career uncertainty",
        "professional doubt", "work-life balance struggle", "occupational stress", "workplace tension",
        "office politics stress", "colleague conflicts", "boss problems", "management issues",
        "team dynamics stress", "project anxiety", "assignment worry", "task overwhelm",
        "responsibility pressure", "financial concerns", "money worries", "budget stress",
        "debt anxiety", "bill pressure", "expense worries", "financial uncertainty", "economic stress",

        # Self-care and coping
        "need self care", "should rest more", "need better sleep", "require more relaxation",
        "should exercise", "need healthier diet", "should meditate", "need mindfulness",
        "require stress management", "need coping strategies", "should seek support", "need to talk",
        "should reach out", "need connection", "require social support", "should join community",
        "need belonging", "require validation", "should practice self-compassion", "need self-acceptance",
        "require self-love", "should set boundaries", "need limits", "require saying no",
        "should prioritize myself", "need me-time", "require personal space", "should decompress",
        "need unwind", "require relaxation", "should take break", "need vacation", "require time off",
        "should slow down", "need pace myself", "require balance", "should simplify", "need reduce stress"
    ],
    "no risk": [
        # Positive and neutral phrases
        "feeling good", "doing well", "happy today", "content with life", "grateful for",
        "looking forward to", "excited about", "optimistic", "hopeful", "positive outlook",
        "managing well", "coping fine", "handling things", "under control", "stable mood",
        "balanced emotions", "peaceful mind", "calm today", "relaxed state", "at ease",
        "comfortable", "secure", "confident", "self-assured", "capable", "competent",
        "productive day", "accomplished tasks", "achieved goals", "making progress",
        "moving forward", "growing", "learning", "improving", "developing skills",
        "healthy habits", "good routine", "consistent schedule", "proper sleep",
        "balanced diet", "regular exercise", "staying active", "physical health",
        "mental wellness", "emotional stability", "spiritual peace", "mindfulness",
        "meditation practice", "yoga routine", "breathing exercises", "stress management",
        "coping strategies", "support system", "good friends", "loving family",
        "healthy relationships", "positive connections", "social support", "community",
        "belonging", "accepted", "understood", "valued", "appreciated", "respected",
        "work satisfaction", "career growth", "professional development", "job security",
        "financial stability", "budget management", "savings plan", "investment growth",
        "personal growth", "self-improvement", "hobbies", "interests", "passions",
        "creative outlets", "artistic expression", "music enjoyment", "reading for pleasure",
        "nature appreciation", "outdoor activities", "exercise enjoyment", "sports participation",
        "travel plans", "vacation anticipation", "future plans", "goal setting",
        "dream chasing", "aspirations", "ambitions", "motivation", "inspiration",
        "gratitude practice", "counting blessings", "appreciating small things",
        "mindful moments", "present focus", "living in the moment", "enjoying now",
        "contentment", "satisfaction", "fulfillment", "purpose", "meaning", "direction",

        # Neutral daily life phrases
        "normal day", "routine activities", "daily tasks", "usual schedule", "regular routine",
        "typical morning", "standard evening", "ordinary week", "common occurrences",
        "mundane tasks", "chores", "errands", "shopping", "cooking", "cleaning",
        "work tasks", "job responsibilities", "professional duties", "meetings", "projects",
        "studying", "learning", "research", "reading", "writing", "planning",
        "organizing", "scheduling", "time management", "priority setting", "decision making",
        "problem solving", "critical thinking", "analysis", "evaluation", "reflection",

        # Mild concerns that don't indicate risk
        "slightly tired", "a bit busy", "somewhat stressed", "minor frustration", "small annoyance",
        "temporary setback", "brief inconvenience", "passing thought", "fleeting worry",
        "manageable stress", "normal pressure", "expected challenge", "routine difficulty",
        "common problem", "typical issue", "standard concern", "regular adjustment",
        "adapting to change", "learning curve", "growth opportunity", "development phase",
        "transition period", "adjustment time", "settling in", "getting used to",
        "becoming familiar", "gaining experience", "building skills", "improving abilities",
        "developing competence", "increasing confidence", "growing capability"
    ]
}

# data=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mental health/Datasets/bert_sentiment_labeled.csv')

# data['content'] = data['content'].fillna('')

# data.info()

!pip install -q gensim

from gensim.models import FastText

sentences = [post.split() for post in data['content']]
ft_model = FastText(sentences, vector_size=100, window=5, min_count=3, workers=4, epochs=10)

# Load models
embed_model = SentenceTransformer("all-MiniLM-L6-v2") # model used for semantic similarity

# Precompute embeddings for all lexicon terms
lexicon_embeddings = {
    level: embed_model.encode(phrases, convert_to_tensor=True)
    for level, phrases in risk_lexicon.items()
}

"""# **Semantic similarity scoring**"""

def get_semantic_risk(text):
    text_emb = embed_model.encode(text, convert_to_tensor=True)

    best_score = -1
    best_risk = "unknown"

    for risk_level, emb_list in lexicon_embeddings.items():
        sims = util.cos_sim(text_emb, emb_list)
        max_sim = torch.max(sims).item()

        if max_sim > best_score:
            best_score = max_sim
            best_risk = risk_level

    return best_risk, best_score

"""# **fine-tuned BERT model to get the risk level and confidence**"""

ensemble = None
id2label = {0: "low", 1: "moderate", 2: "high", 3: "no_risk"}  # Your label mapping

def initialize_ensemble():
    """Initialize the ensemble model (call this once at startup)"""
    global ensemble
    save_dir = "/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/Models/prod_ensemble_v1"
    # save_dir = "/content/drive/MyDrive/Colab Notebooks/Mental Health Project/Models/prod_ensemble_v1"
    ensemble = load_ensemble(save_dir)
    print("✅ Ensemble loaded successfully")
    return ensemble

def get_ensemble_prediction(text):
    """
    Get final ensemble prediction with confidence score.
    Returns: risk label and confidence score
    """
    risk_labels, confidences = ensemble.predict(text)

    if isinstance(text, str):
        return risk_labels[0], confidences[0]

    return risk_labels, confidences

"""# **Function to find out Combined Prediction**"""

def combine_predictions(model_risk, model_conf, semantic_risk, sim_score,
                        sim_threshold=0.7, model_conf_threshold=0.85):
    if model_risk != semantic_risk:
        if sim_score >= sim_threshold and model_conf < model_conf_threshold:
            return semantic_risk
    return model_risk

"""# **Final Prediction**"""

def refined_risk_level_with_semantic_voting(text):
    model_risk, model_conf = get_ensemble_prediction(text)
    semantic_risk, sim_score = get_semantic_risk(text)
    final_risk = combine_predictions(model_risk, model_conf, semantic_risk, sim_score)

    return {
        "text": text,
        "model_predicted_risk": model_risk,
        "model_confidence": round(model_conf, 4),
        "semantic_risk": semantic_risk,
        "semantic_similarity_score": round(sim_score, 4),
        "final_risk": final_risk
    }

"""# **Performance**"""

import seaborn as sns

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from tqdm import tqdm
import numpy as np

def analyze_hybrid_decisions(true_labels, ensemble_preds, hybrid_preds,
                           model_confidences, semantic_scores):
    """
    Analyze when the hybrid approach changes the prediction
    """
    changed_count = 0
    correct_changes = 0
    incorrect_changes = 0

    print("=" * 60)
    print("HYBRID DECISION ANALYSIS:")
    print("=" * 60)

    for i, (true, ensemble, hybrid) in enumerate(zip(true_labels, ensemble_preds, hybrid_preds)):
        if ensemble != hybrid:
            changed_count += 1
            if hybrid == true:
                correct_changes += 1
            else:
                incorrect_changes += 1

            # Show details for the first few changes
            if changed_count <= 5:
                print(f"Text {i+1}: Ensemble={ensemble}({model_confidences[i]:.3f}) -> Hybrid={hybrid}")
                print(f"  Semantic score: {semantic_scores[i]:.3f}, True label: {true}")
                print(f"  Change: {'CORRECT' if hybrid == true else 'INCORRECT'}")
                print()

    print(f"Total predictions changed by hybrid approach: {changed_count}")
    print(f"Correct changes: {correct_changes} ({correct_changes/max(1, changed_count)*100:.1f}%)")
    print(f"Incorrect changes: {incorrect_changes} ({incorrect_changes/max(1, changed_count)*100:.1f}%)")
    print(f"Net improvement: {correct_changes - incorrect_changes}")

    if changed_count > 0:
        print(f"Overall impact on accuracy: {(correct_changes - incorrect_changes) / len(true_labels) * 100:.2f}%")

def evaluate_hybrid_performance(X_test, y_test, ensemble_model, id2label):
    """
    Evaluate the performance of your hybrid approach using the true test labels
    """
    # Convert true labels to risk level names
    true_labels = [id2label[label] for label in y_test]

    # Initialize lists to store results
    model_only_preds = []
    semantic_only_preds = []
    hybrid_preds = []

    model_confidences = []
    semantic_scores = []

    print("Evaluating hybrid performance on test set...")

    # Process each text
    for i, text in enumerate(tqdm(X_test, desc="Processing test texts")):
        if not text or pd.isna(text):  # Skip empty texts
            continue

        # Get model prediction
        model_risk, model_conf = get_ensemble_prediction(text)
        model_only_preds.append(model_risk)
        model_confidences.append(model_conf)

        # Get semantic prediction
        semantic_risk, sim_score = get_semantic_risk(text)
        semantic_only_preds.append(semantic_risk)
        semantic_scores.append(sim_score)

        # Get hybrid prediction
        hybrid_result = refined_risk_level_with_semantic_voting(text)
        hybrid_preds.append(hybrid_result["final_risk"])

    # Debug: Check unique values in all predictions
    print("=" * 60)
    print("DEBUG - UNIQUE CLASSES FOUND:")
    print("=" * 60)
    print(f"True labels unique: {sorted(set(true_labels))}")
    print(f"Model preds unique: {sorted(set(model_only_preds))}")
    print(f"Semantic preds unique: {sorted(set(semantic_only_preds))}")
    print(f"Hybrid preds unique: {sorted(set(hybrid_preds))}")

    # Get the actual classes that exist in true labels (this should be your reference)
    true_classes = sorted(set(true_labels))
    print(f"True classes to evaluate: {true_classes}")
    print("=" * 60)

    # Calculate performance metrics
    print("=" * 60)
    print("HYBRID MODEL EVALUATION ON TEST SET")
    print("=" * 60)

    print("ENSEMBLE MODEL ONLY PERFORMANCE:")
    ensemble_report = classification_report(true_labels, model_only_preds,
                                           labels=true_classes,
                                           target_names=true_classes,
                                           zero_division=0)
    print(ensemble_report)

    print("SEMANTIC ONLY PERFORMANCE:")
    semantic_report = classification_report(true_labels, semantic_only_preds,
                                           labels=true_classes,  # Use true classes as reference
                                           target_names=true_classes,
                                           zero_division=0)
    print(semantic_report)

    print("HYBRID (MODEL + SEMANTIC) PERFORMANCE:")
    hybrid_report = classification_report(true_labels, hybrid_preds,
                                         labels=true_classes,
                                         target_names=true_classes,
                                         zero_division=0)
    print(hybrid_report)

    # Calculate accuracy scores
    ensemble_accuracy = accuracy_score(true_labels, model_only_preds)
    semantic_accuracy = accuracy_score(true_labels, semantic_only_preds)
    hybrid_accuracy = accuracy_score(true_labels, hybrid_preds)

    print("ACCURACY COMPARISON:")
    print(f"Ensemble Only Accuracy: {ensemble_accuracy:.4f}")
    print(f"Semantic Only Accuracy: {semantic_accuracy:.4f}")
    print(f"Hybrid Accuracy: {hybrid_accuracy:.4f}")
    print(f"Improvement over Ensemble: {hybrid_accuracy - ensemble_accuracy:.4f}")
    print(f"Improvement over Semantic: {hybrid_accuracy - semantic_accuracy:.4f}")

    # Create confusion matrices using true_classes as reference
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # Ensemble confusion matrix
    ensemble_cm = confusion_matrix(true_labels, model_only_preds, labels=true_classes)
    sns.heatmap(ensemble_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                xticklabels=true_classes,
                yticklabels=true_classes)
    axes[0].set_title('Ensemble Only Confusion Matrix')
    axes[0].set_xlabel('Predicted')
    axes[0].set_ylabel('True')

    # Semantic confusion matrix
    semantic_cm = confusion_matrix(true_labels, semantic_only_preds, labels=true_classes)
    sns.heatmap(semantic_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],
                xticklabels=true_classes,
                yticklabels=true_classes)
    axes[1].set_title('Semantic Only Confusion Matrix')
    axes[1].set_xlabel('Predicted')
    axes[1].set_ylabel('True')

    # Hybrid confusion matrix
    hybrid_cm = confusion_matrix(true_labels, hybrid_preds, labels=true_classes)
    sns.heatmap(hybrid_cm, annot=True, fmt='d', cmap='Blues', ax=axes[2],
                xticklabels=true_classes,
                yticklabels=true_classes)
    axes[2].set_title('Hybrid Confusion Matrix')
    axes[2].set_xlabel('Predicted')
    axes[2].set_ylabel('True')

    plt.tight_layout()
    plt.show()

    # Analyze when the hybrid approach makes a difference
    analyze_hybrid_decisions(true_labels, model_only_preds, hybrid_preds,
                            model_confidences, semantic_scores)

    return {
        "true_labels": true_labels,
        "ensemble_preds": model_only_preds,
        "semantic_preds": semantic_only_preds,
        "hybrid_preds": hybrid_preds,
        "ensemble_accuracy": ensemble_accuracy,
        "semantic_accuracy": semantic_accuracy,
        "hybrid_accuracy": hybrid_accuracy,
        "model_confidences": model_confidences,
        "semantic_scores": semantic_scores,
        "true_classes": true_classes
    }

ensemble = initialize_ensemble()

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt

# Evaluate on the test set
test_results = evaluate_hybrid_performance(X_test, y_test, ensemble, id2label
                                           )

import pickle
pickle.dump(test_results, open('/content/drive/MyDrive/test_results.pkl', 'wb'))

def detailed_analysis_by_risk_level(test_results, id2label):
    """
    Perform detailed analysis of performance by risk level
    """
    true_labels = test_results["true_labels"]
    ensemble_preds = test_results["ensemble_preds"]
    hybrid_preds = test_results["hybrid_preds"]

    risk_levels = ["high", "moderate", "low", "no risk"]

    # Calculate performance for each risk level
    results = {}

    for level in risk_levels:
        # Create mask for this risk level
        mask = [tl == level for tl in true_labels]

        if sum(mask) > 0:
            # Get predictions for this risk level
            true_subset = [tl for tl, m in zip(true_labels, mask) if m]
            ensemble_subset = [ep for ep, m in zip(ensemble_preds, mask) if m]
            hybrid_subset = [hp for hp, m in zip(hybrid_preds, mask) if m]

        # Calculate accuracy
        ensemble_acc = accuracy_score(true_subset, ensemble_subset)
        hybrid_acc = accuracy_score(true_subset, hybrid_subset)

        # Count changes
        changes = sum(1 for ep, hp in zip(ensemble_subset, hybrid_subset) if ep != hp)
        correct_changes = sum(1 for ep, hp, tl in zip(ensemble_subset, hybrid_subset, true_subset)
                            if ep != hp and hp == tl)
        incorrect_changes = sum(1 for ep, hp, tl in zip(ensemble_subset, hybrid_subset, true_subset)
                              if ep != hp and hp != tl)

        results[level] = {
            "count": len(true_subset),
            "ensemble_accuracy": ensemble_acc,
            "hybrid_accuracy": hybrid_acc,
            "accuracy_change": hybrid_acc - ensemble_acc,
            "predictions_changed": changes,
            "correct_changes": correct_changes,
            "incorrect_changes": incorrect_changes,
            "net_improvement": correct_changes - incorrect_changes
        }

    # Print results
    print("DETAILED ANALYSIS BY RISK LEVEL:")
    print("=" * 60)
    for level, stats in results.items():
        print(f"{level.upper()} RISK (n={stats['count']}):")
        print(f"  Ensemble Accuracy: {stats['ensemble_accuracy']:.3f}")
        print(f"  Hybrid Accuracy: {stats['hybrid_accuracy']:.3f}")
        print(f"  Change: {stats['accuracy_change']:+.3f}")
        print(f"  Predictions Changed: {stats['predictions_changed']}")
        print(f"  Correct Changes: {stats['correct_changes']}")
        print(f"  Incorrect Changes: {stats['incorrect_changes']}")
        print(f"  Net Improvement: {stats['net_improvement']:+.0f}")
        print()

    # Plot results
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Accuracy by risk level
    x = np.arange(len(risk_levels))
    width = 0.35

    model_accs = [results[level]['ensemble_accuracy'] for level in risk_levels]
    hybrid_accs = [results[level]['hybrid_accuracy'] for level in risk_levels]

    axes[0].bar(x - width/2, model_accs, width, label='Ensemble Only', alpha=0.7)
    axes[0].bar(x + width/2, hybrid_accs, width, label='Hybrid', alpha=0.7)
    axes[0].set_xlabel('Risk Level')
    axes[0].set_ylabel('Accuracy')
    axes[0].set_title('Accuracy by Risk Level')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(risk_levels)
    axes[0].legend()
    axes[0].grid(True, axis='y')

    # Net improvement by risk level
    improvements = [results[level]['net_improvement'] for level in risk_levels]
    colors = ['red' if imp < 0 else 'green' for imp in improvements]
    axes[1].bar(x, improvements, color=colors)
    axes[1].set_xlabel('Risk Level')
    axes[1].set_ylabel('Net Improvement')
    axes[1].set_title('Net Improvement by Risk Level')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(risk_levels)
    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
    axes[1].grid(True, axis='y')

    plt.tight_layout()
    plt.show()

    return results

# Run detailed analysis
risk_level_results = detailed_analysis_by_risk_level(test_results, id2label)

def analyze_threshold_performance(test_results, X_test, y_test):
    """
    Analyze how different thresholds affect hybrid performance
    """
    import pandas as pd
    true_labels = [id2label[label] for label in y_test]

    # Test different threshold combinations
    sim_thresholds = [0.5, 0.6, 0.65, 0.7, 0.75, 0.8]
    conf_thresholds = [0.7, 0.75, 0.8, 0.85, 0.9]

    results = []

    print("Analyzing threshold performance...")

    for sim_thresh in tqdm(sim_thresholds, desc="Similarity thresholds"):
        for conf_thresh in conf_thresholds:
            hybrid_preds = []

            for text in X_test:
                if not text or pd.isna(text):
                    continue

                model_risk, model_conf = get_ensemble_prediction(text)
                semantic_risk, sim_score = get_semantic_risk(text)

                # Apply custom thresholds
                if model_conf >= conf_thresh:
                    final_risk = model_risk
                elif sim_score >= sim_thresh:
                    final_risk = semantic_risk
                else:
                    final_risk = model_risk

                hybrid_preds.append(final_risk)

            accuracy = accuracy_score(true_labels, hybrid_preds)
            results.append({
                'sim_threshold': sim_thresh,
                'conf_threshold': conf_thresh,
                'accuracy': accuracy
            })

    # Find best thresholds
    best_result = max(results, key=lambda x: x['accuracy'])

    print(f"Best thresholds: sim_threshold={best_result['sim_threshold']}, conf_threshold={best_result['conf_threshold']}")
    print(f"Best accuracy: {best_result['accuracy']:.4f}")

    # Create heatmap
    import pandas as pd
    results_df = pd.DataFrame(results)
    pivot_df = results_df.pivot(index='sim_threshold', columns='conf_threshold', values='accuracy')

    plt.figure(figsize=(10, 6))
    sns.heatmap(pivot_df, annot=True, fmt='.4f', cmap='YlOrRd')
    plt.title('Hybrid Accuracy with Different Thresholds')
    plt.xlabel('Confidence Threshold')
    plt.ylabel('Similarity Threshold')
    plt.show()

    return results, best_result

# Run threshold analysis
threshold_results, best_thresholds = analyze_threshold_performance(test_results, X_test, y_test)

def comprehensive_hybrid_evaluation(X_test, y_test, ensemble_model, id2label):
    """
    Run a comprehensive evaluation of the hybrid approach
    """
    print("COMPREHENSIVE HYBRID MODEL EVALUATION")
    print("=" * 60)
    print(f"Using test set with {len(X_test)} samples")
    print("=" * 60)

    # Run all analyses
    test_results = evaluate_hybrid_performance(X_test, y_test, ensemble_model, id2label)
    risk_level_results = detailed_analysis_by_risk_level(test_results, id2label)
    threshold_results, best_thresholds = analyze_threshold_performance(test_results, X_test, y_test)

    # Summary
    print("\n" + "=" * 60)
    print("FINAL SUMMARY")
    print("=" * 60)

    print(f"Overall Hybrid Accuracy: {test_results['hybrid_accuracy']:.4f}")
    print(f"Improvement over Ensemble: {test_results['hybrid_accuracy'] - test_results['ensemble_accuracy']:.4f}")

    # Find which risk levels benefited most
    max_improvement = -1
    best_level = None
    for level, stats in risk_level_results.items():
        if stats['accuracy_change'] > max_improvement:
            max_improvement = stats['accuracy_change']
            best_level = level

    if best_level:
        print(f"Biggest improvement for: {best_level} risk (+{max_improvement:.3f})")

    print(f"Optimal thresholds: sim_threshold={best_thresholds['sim_threshold']}, conf_threshold={best_thresholds['conf_threshold']}")

    return {
        "test_results": test_results,
        "risk_level_results": risk_level_results,
        "threshold_results": threshold_results,
        "best_thresholds": best_thresholds
    }

# Run comprehensive evaluation
final_results = comprehensive_hybrid_evaluation(X_test, y_test, ensemble, id2label)

"""# **RESULT**"""

# Apply to all posts and store results
results = []
ensemble = initialize_ensemble()
if ensemble is None:
    raise RuntimeError("Failed to load ensemble model")
for text in data['content']:
    try:
        result = refined_risk_level_with_semantic_voting(text)
        results.append(result)
    except Exception as e:
        print(f"Error processing text: {text[:60]}... | Error: {e}")

results_df = pd.DataFrame(results)

results_df.to_csv("results_df.csv", index=False)

results_df.info()

results_df.sample(5)

results_df['text'].iloc[8626]

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(results_df['semantic_similarity_score  '], kde=True, bins=20, color='skyblue')
plt.title("Distribution of semantic_similarity_score  ")
plt.xlabel("semantic_similarity_score  ")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

results_df['final_risk'].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

# Count the number of posts per risk level (for all present categories)
risk = results_df['final_risk'].value_counts()

# Plot
plt.figure(figsize=(5, 3.5))  # Slightly larger to fit 4 categories
sns.barplot(x=risk.index, y=risk.values, palette="coolwarm")
plt.title("Distribution of risk")
plt.xlabel("Risks")
plt.ylabel("Number of Posts")
plt.tight_layout()
plt.show()

confusion_df = pd.crosstab(results_df['model_predicted_risk'], results_df['semantic_risk'], rownames=['Model'], colnames=['Semantic'])
sns.heatmap(confusion_df, annot=True, cmap="Blues", fmt='d')
plt.title("Agreement Between Model and Semantic Risk")
plt.tight_layout()
plt.show()

print(confusion_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Count the number of posts per risk level (for all present categories)
risk = data['final_risk'].value_counts()

# Plot
plt.figure(figsize=(5, 3.5))  # Slightly larger to fit 4 categories
sns.barplot(x=risk.index, y=risk.values, palette="coolwarm")
plt.title("Distribution of risk")
plt.xlabel("Risks")
plt.ylabel("Number of Posts")
plt.tight_layout()
plt.show()

avg_confidence = data.groupby('bert_sentiment')['bert_sentiment_confidence'].mean().sort_values()
print(avg_confidence)

print(data['Sentiment'].value_counts())
print(data['label'].value_counts())

# Step 1: Import required libraries
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

# Step 2: Define a text cleaning function for better word clouds
def clean_for_wordcloud(text):
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)  # Remove links
    text = re.sub(r"@\w+|#\w+", "", text)                # Remove mentions and hashtags
    text = re.sub(r"[^A-Za-z\s]", "", text)              # Remove punctuation/numbers
    text = text.lower()                                  # Convert to lowercase
    return text

# Step 3: Loop through each unique risk level in your data
for risk_level in sorted(results_df['final_risk'].unique()):
    # Step 4: Clean the text for that risk level (skip NaNs)
    texts = results_df[results_df['final_risk'] == risk_level]['text'].dropna().apply(clean_for_wordcloud)

    # Step 5: Combine all cleaned posts into one string
    all_text = " ".join(texts)

    # Step 6: Generate the word cloud
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        stopwords=STOPWORDS
    ).generate(all_text)

    # Step 7: Plot the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for {risk_level.capitalize()} Risk")
    plt.show()

"""## **Extraction Locations and Classifying them on the basis of risk**"""

results=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mental health/Mental Health Project/testing/results_df.csv')
data=results.copy()
data.rename(columns={'text':'content','final_risk':'risk'},inplace=True)

data.info()

import pandas as pd
from transformers import pipeline
from collections import defaultdict
from geopy.geocoders import Nominatim
import folium
from folium.plugins import MarkerCluster
import time


# Step 2: Load NER pipeline
ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

# Step 3: Location extractor
def extract_locations_from_text(text):
    try:
        ner_results = ner_pipeline(text)
        locations = []
        current_location = ''
        for entity in ner_results:
            if entity['entity_group'] == 'LOC':
                if current_location:
                    current_location += ' ' + entity['word']
                else:
                    current_location = entity['word']
            else:
                if current_location:
                    locations.append(current_location.strip())
                    current_location = ''
        if current_location:
            locations.append(current_location.strip())
        return locations
    except:
        return []

# Step 4: Add 'location' column
data['location'] = data['content'].apply(lambda text: ", ".join(extract_locations_from_text(text)) if isinstance(text, str) and text.strip() else None)

# Step 5: Group risk levels by location
location_risks = defaultdict(list)
for _, row in data.iterrows():
    if row['location']:
        for loc in row['location'].split(", "):  # multiple locations in one string
            location_risks[loc].append(row['risk'])

# Step 6: Get majority risk per location
def get_majority_risk(risks):
    return max(set(risks), key=risks.count)

final_location_risk = {
    loc: get_majority_risk(risk_list) for loc, risk_list in location_risks.items() if risk_list
}

# Step 7: Geocode locations
geolocator = Nominatim(user_agent="mental-health-locator")
location_coords = {}
for loc in final_location_risk:
    try:
        geo = geolocator.geocode(loc)
        if geo:
            location_coords[loc] = (geo.latitude, geo.longitude)
        time.sleep(1)  # Avoid rate limits
    except:
        continue

# Step 8: Create DataFrame for mapping
map_data = []
for loc, coords in location_coords.items():
    map_data.append({
        "location": loc,
        "latitude": coords[0],
        "longitude": coords[1],
        "risk": final_location_risk[loc]
    })

map_df = pd.DataFrame(map_data)

# Optional: Save map-ready CSV
# map_df.to_csv("map_risk_data.csv", index=False)

# Step 9: Draw Heatmap (Folium)
color_map = {
    "high": "red",
    "moderate": "orange",
    "low": "yellow",
    "no risk": "green"
}

m = folium.Map(location=[20.5937, 78.9629], zoom_start=4)  # Centered over India
marker_cluster = MarkerCluster().add_to(m)

for _, row in map_df.iterrows():
    folium.CircleMarker(
        location=(row['latitude'], row['longitude']),
        radius=6,
        color=color_map.get(row['risk'], "gray"),
        fill=True,
        fill_opacity=0.8,
        popup=f"{row['location']} ({row['risk']})"
    ).add_to(marker_cluster)

# Save map to HTML
m.save("location_risk_map.html")
print("✅ Map saved as location_risk_map.html")

data.info()

"""## Ploting **Heatmap**"""

import folium
from folium.plugins import HeatMap
import requests

# Function to get latitude and longitude using Geonames API
def geocode_location(location_name):
    username = "gunjan_2311"  # Replace with your actual GeoNames username
    url = f"http://api.geonames.org/searchJSON?q={location_name}&maxRows=1&username={username}"
    response = requests.get(url)

    try:
        data = response.json()
        if data['geonames']:
            latitude = float(data['geonames'][0]['lat'])
            longitude = float(data['geonames'][0]['lng'])
            return latitude, longitude
    except:
        pass

    return None, None  # Return None if location is not found or API fails

# Function to create the heatmap
def create_heatmap_with_risk(text, tokenizer, model):
    # Extract locations and classify risk
    location_risks = unique_locations

    # Initialize Folium map centered globally
    m = folium.Map(location=[20, 0], zoom_start=2)

    # Prepare data for heatmap
    heat_data = []

    for location, risk in location_risks.items():
        lat, lon = geocode_location(location)

        if lat and lon:
            # Assign intensity based on 4-level risk
            if risk == "high":
                intensity = 1.0
            elif risk == "moderate":
                intensity = 0.66
            elif risk == "low":
                intensity = 0.33
            elif risk == "no risk":
                intensity = 0.1
            else:
                continue  # Skip unknown risk

            heat_data.append([lat, lon, intensity])

            # Optional: Add marker for debugging or clarity
            folium.CircleMarker(
                location=[lat, lon],
                radius=6,
                popup=f"{location} ({risk})",
                color="red" if risk == "high" else "orange" if risk == "moderate" else "blue" if risk == "low" else "green",
                fill=True,
                fill_opacity=0.6
            ).add_to(m)

    # Add heat layer
    HeatMap(heat_data, radius=15, blur=10, max_zoom=1).add_to(m)

    # Save map
    m.save("location_risk_heatmap.html")
    return m

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Mental Health Project/testing/bert_sentiment_labeled (1).csv")

result = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Mental Health Project/testing/results_df.csv")

result.info()

data.info()

data.head()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Step 1: Read your CSV file (if not already loaded)
# data = pd.read_csv("your_file.csv")  # Skip if already loaded

# Step 2: Convert 'timestamp' to datetime
data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')

# Step 3: Extract the weekday name
data['weekday'] = data['timestamp'].dt.day_name()

# Step 4: Group by weekday and sentiment
sentiment_by_day = data.groupby(['weekday', 'bert_sentiment']).size().reset_index(name='count')

# Step 5: Set weekday order
weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
sentiment_by_day['weekday'] = pd.Categorical(sentiment_by_day['weekday'], categories=weekday_order, ordered=True)
sentiment_by_day = sentiment_by_day.sort_values('weekday')

# Step 6: Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=sentiment_by_day, x='weekday', y='count', hue='bert_sentiment', palette='Set2')
plt.title('Sentiment Distribution by Weekday')
plt.xlabel('Day of Week')
plt.ylabel('Number of Posts')
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Convert Unix timestamp to datetime object
data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')

# 2. Now extract the hour from the datetime
data['hour'] = data['timestamp'].dt.hour

# 3. Group by hour and sentiment
hourly_sentiment = data.groupby(['hour', 'bert_sentiment']).size().reset_index(name='count')

# 4. Plot line chart
plt.figure(figsize=(12, 6))
sns.lineplot(data=hourly_sentiment, x='hour', y='count', hue='bert_sentiment', marker='o', palette='Set2')
plt.title('Sentiment Distribution by Hour of the Day')
plt.xlabel('Hour of Day (0 to 23)')
plt.ylabel('Number of Posts')
plt.xticks(range(0, 24))
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Convert timestamp (if not already)
data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')

# 2. Extract hour from timestamp
data['hour'] = data['timestamp'].dt.hour

# 3. Define time-of-day buckets
def get_time_of_day(hour):
    if 5 <= hour < 12:
        return 'Morning (5AM–12PM)'
    elif 12 <= hour < 17:
        return 'Afternoon (12PM–5PM)'
    elif 17 <= hour < 21:
        return 'Evening (5PM–9PM)'
    else:
        return 'Night (9PM–5AM)'

data['time_of_day'] = data['hour'].apply(get_time_of_day)

# 4. Group by time of day and sentiment
grouped = data.groupby(['time_of_day', 'bert_sentiment']).size().reset_index(name='count')

# 5. Ensure logical order of time buckets
time_order = ['Morning (5AM–12PM)', 'Afternoon (12PM–5PM)', 'Evening (5PM–9PM)', 'Night (9PM–5AM)']
grouped['time_of_day'] = pd.Categorical(grouped['time_of_day'], categories=time_order, ordered=True)
grouped = grouped.sort_values('time_of_day')

# 6. Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=grouped, x='time_of_day', y='count', hue='bert_sentiment', palette='Set2')
plt.title('Sentiment Distribution by Time of Day')
plt.xlabel('Time of Day')
plt.ylabel('Number of Posts')
plt.xticks(rotation=15)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Convert timestamp to datetime
data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')

# 2. Extract weekday (0=Monday, 6=Sunday) and hour
data['weekday'] = data['timestamp'].dt.day_name()
data['hour'] = data['timestamp'].dt.hour

# 3. Map hour to time of day
def get_time_of_day(hour):
    if 5 <= hour < 12:
        return 'Morning (5AM–12PM)'
    elif 12 <= hour < 17:
        return 'Afternoon (12PM–5PM)'
    elif 17 <= hour < 21:
        return 'Evening (5PM–9PM)'
    else:
        return 'Night (9PM–5AM)'

data['time_of_day'] = data['hour'].apply(get_time_of_day)

# 4. Plot 1: Posts per weekday
plt.figure(figsize=(8, 5))
sns.countplot(data=data, x='weekday', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], palette='pastel')
plt.title('Number of Posts per Weekday')
plt.xticks(rotation=45)
plt.xlabel('Weekday')
plt.ylabel('Number of Posts')
plt.tight_layout()
plt.show()

# 5. Plot 2: Posts per time of day
time_order = ['Morning (5AM–12PM)', 'Afternoon (12PM–5PM)', 'Evening (5PM–9PM)', 'Night (9PM–5AM)']
plt.figure(figsize=(8, 5))
sns.countplot(data=data, x='time_of_day', order=time_order, palette='Set2')
plt.title('Number of Posts by Time of Day')
plt.xticks(rotation=15)
plt.xlabel('Time of Day')
plt.ylabel('Number of Posts')
plt.tight_layout()
plt.show()

# 6. Plot 3: Heatmap of Weekday × Time of Day
pivot = data.groupby(['weekday', 'time_of_day']).size().unstack().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])

plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Posts by Weekday and Time of Day')
plt.xlabel('Time of Day')
plt.ylabel('Weekday')
plt.tight_layout()
plt.show()